{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKhFh6nI8lMuMBCd4lbTZ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wahiba275/CBOW-Skip-gram-from-scratch/blob/main/CBOW_%26_Skip_gram_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus1 =[\"Le match 'hier était incroyable\" , \"C'était un match mémorable\" , \"La récente allocution du premier ministre\"]"
      ],
      "metadata": {
        "id": "TLfBR05cKfhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***CBOW***"
      ],
      "metadata": {
        "id": "cyOJF6YomU3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def tokenize_corpus(corpus):\n",
        "    tokenized_corpus = [sentence.split() for sentence in corpus]\n",
        "    word_to_index = {}\n",
        "    index_to_word = {}\n",
        "    for sentence in tokenized_corpus:\n",
        "        for word in sentence:\n",
        "            if word not in word_to_index:\n",
        "                index = len(word_to_index)\n",
        "                word_to_index[word] = index\n",
        "                index_to_word[index] = word\n",
        "    return tokenized_corpus, word_to_index, index_to_word"
      ],
      "metadata": {
        "id": "ylpMDjKQKfkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus, word_to_index, index_to_word=tokenize_corpus(corpus1)\n",
        "print(index_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96UF03rSbXS5",
        "outputId": "a783d222-1b84-4da5-e51f-7c000f0f3577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'Le', 1: 'match', 2: \"'hier\", 3: 'était', 4: 'incroyable', 5: \"C'était\", 6: 'un', 7: 'mémorable', 8: 'La', 9: 'récente', 10: 'allocution', 11: 'du', 12: 'premier', 13: 'ministre'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(corpus, window_size, V, word_to_index):\n",
        "    training_data = []\n",
        "    for sentence in corpus:\n",
        "        sentence_length = len(sentence)\n",
        "        for target_word_index, target_word in enumerate(sentence):\n",
        "            context = []\n",
        "            start = max(0, target_word_index - window_size)\n",
        "            end = min(sentence_length, target_word_index + window_size + 1)\n",
        "            context = [sentence[i] for i in range(start, end) if i != target_word_index]\n",
        "            target = np.zeros(V)\n",
        "            target[word_to_index[target_word]] = 1\n",
        "            training_data.append((context, target))\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "xhEZXyMZLD-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(V, N):\n",
        "    context_weights = np.random.rand(V, N)\n",
        "    target_weights = np.random.rand(N, V)\n",
        "    return context_weights, target_weights\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n"
      ],
      "metadata": {
        "id": "aGUS9DUnLY8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_CBOW(corpus, V, N, epochs, learning_rate):\n",
        "    tokenized_corpus, word_to_index, index_to_word = tokenize_corpus(corpus)\n",
        "    context_weights, target_weights = initialize_weights(V, N)\n",
        "\n",
        "    losses = []  # List to store loss values for each epoch\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss = 0\n",
        "\n",
        "        for context, target in generate_training_data(tokenized_corpus, window_size=1, V=V, word_to_index=word_to_index):\n",
        "            context_vector = np.mean([context_weights[word_to_index[word]] for word in context], axis=0)\n",
        "            predicted_target = softmax(np.dot(context_vector, target_weights))\n",
        "\n",
        "            # Calcul de la perte\n",
        "            loss += -np.sum(target * np.log(predicted_target))\n",
        "\n",
        "            # Mise à jour des poids\n",
        "            error = predicted_target - target\n",
        "            target_weights -= learning_rate * np.outer(context_vector, error)\n",
        "            for word in context:\n",
        "                context_weights[word_to_index[word]] -= learning_rate * np.dot(error, target_weights.T[:, word_to_index[word]])\n",
        "\n",
        "        print(f'Epoch: {epoch+1}, Loss: {loss}')\n",
        "        losses.append(loss)\n",
        "    return context_weights, word_to_index, losses\n",
        "\n",
        "# Exemple d'utilisation avec votre corpus\n",
        "corpus = [\n",
        "    \"trouver bonne assurance\",\n",
        "    \"contrat satisfaisant\",\n",
        "    \"changement contrat assurance\"\n",
        "]\n",
        "V = 6  # Taille du vocabulaire\n",
        "N = 6  # Dimension des vecteurs de mots\n",
        "epochs = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "context_weights, word_to_index , losses  = train_CBOW(corpus, V, N, epochs, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApRwACN6MZDf",
        "outputId": "aa320bd5-69d7-4cc8-de99-f5cfef267078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 14.959915613863211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YgSOstkZUbp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "    dot_product = np.dot(v1, v2)\n",
        "    norm_v1 = np.linalg.norm(v1)\n",
        "    norm_v2 = np.linalg.norm(v2)\n",
        "    similarity = dot_product / (norm_v1 * norm_v2)\n",
        "    return similarity\n",
        "\n",
        "# Example words\n",
        "word1 = \"trouver\"\n",
        "word2 = \"assurance\"\n",
        "word3 = \"contrat\"\n",
        "\n",
        "# Get word vectors\n",
        "vector1 = context_weights[word_to_index[word1]]\n",
        "vector2 = context_weights[word_to_index[word2]]\n",
        "vector3 = context_weights[word_to_index[word3]]\n",
        "\n",
        "# Compute cosine similarities\n",
        "similarity_1_2 = cosine_similarity(vector1, vector2)\n",
        "similarity_1_3 = cosine_similarity(vector1, vector3)\n",
        "\n",
        "print(f\"Similarity between '{word1}' and '{word2}': {similarity_1_2}\")\n",
        "print(f\"Similarity between '{word1}' and '{word3}': {similarity_1_3}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snTOrORlMrDc",
        "outputId": "e3400217-ae6a-4e2e-844e-495931fa8281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'trouver' and 'assurance': 0.4084681655856083\n",
            "Similarity between 'trouver' and 'contrat': 0.7995828034394701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Votre corpus\n",
        "corpus2 = [\n",
        "    \"trouver bonne assurance\",\n",
        "    \"contrat satisfaisant\",\n",
        "    \"changement contrat assurance\"\n",
        "]\n",
        "\n",
        "# Tokenisation du corpus en mots\n",
        "tokenized_corpus = [word_tokenize(sentence) for sentence in corpus2]\n",
        "\n",
        "# Création du modèle CBOW\n",
        "model_cbow = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=0, min_count=1)\n",
        "# Entraînement du modèle\n",
        "model_cbow.train(tokenized_corpus, total_examples=len(corpus2), epochs=100)\n",
        "\n",
        "# Vous pouvez maintenant utiliser le modèle entraîné pour effectuer différentes opérations, telles que la similarité entre les mots.\n",
        "similarity_score = model_cbow.wv.similarity('trouver', 'assurance')\n",
        "print(f\"Cosine similarity between 'trouver' and 'assurance' - CBOW : {similarity_score}\")\n",
        "similarity_score2 = model_cbow.wv.similarity('trouver', 'contrat')\n",
        "print(f\"Cosine similarity between 'trouver' and 'contrat' - CBOW : {similarity_score2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xs3x_HHaVZE",
        "outputId": "7c5cdf8a-e9d3-4c94-8698-d480e94bbb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'trouver' and 'assurance' - CBOW : 0.009510169737040997\n",
            "Cosine similarity between 'trouver' and 'contrat' - CBOW : -0.059844862669706345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Skip-gram***"
      ],
      "metadata": {
        "id": "oRhzpKi6maNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Function to preprocess the text data\n",
        "def preprocess_text(corpus):\n",
        "    tokens = [sentence.lower().split() for sentence in corpus]\n",
        "    return tokens\n",
        "\n",
        "# Function to create the skip-gram training pairs\n",
        "def generate_skipgram_pairs(tokens, window_size=2):\n",
        "    pairs = []\n",
        "    for sentence in tokens:\n",
        "        for i, target_word in enumerate(sentence):\n",
        "            context = sentence[max(0, i - window_size):i] + sentence[i+1:i+window_size+1]\n",
        "            for context_word in context:\n",
        "                pairs.append((target_word, context_word))\n",
        "    return pairs\n",
        "\n",
        "# Function to convert words to one-hot vectors\n",
        "def word_to_onehot(word, word2index):\n",
        "    onehot = np.zeros(len(word2index))\n",
        "    onehot[word2index[word]] = 1\n",
        "    return onehot\n",
        "\n",
        "# Skip-gram training function\n",
        "def train_skipgram(tokens, embedding_size=100, window_size=2, epochs=100, learning_rate=0.01):\n",
        "    vocabulary = list(set(word for sentence in tokens for word in sentence))\n",
        "    word2index = {word: i for i, word in enumerate(vocabulary)}\n",
        "    index2word = {i: word for word, i in word2index.items()}\n",
        "    vocab_size = len(vocabulary)\n",
        "\n",
        "    W_input = np.random.uniform(-1, 1, (vocab_size, embedding_size))\n",
        "    W_output = np.random.uniform(-1, 1, (embedding_size, vocab_size))\n",
        "    #losses=[]\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for target, context in generate_skipgram_pairs(tokens, window_size):\n",
        "            target_onehot = word_to_onehot(target, word2index)\n",
        "            y = np.dot(W_input[word2index[target]], W_output)\n",
        "            y_pred = 1 / (1 + np.exp(-y))\n",
        "\n",
        "            error = y_pred - target_onehot\n",
        "            total_loss += np.sum(error**2)\n",
        "            W_output -= learning_rate * np.outer(W_input[word2index[target]], error)\n",
        "            W_input[word2index[target]] -= learning_rate * np.dot(W_output, error)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\")\n",
        "    return W_input, word2index, index2word\n",
        "\n",
        "# Test the skip-gram model\n",
        "corpus1 = [\n",
        "    \"trouver bonne assurance\",\n",
        "    \"contrat satisfaisant\",\n",
        "    \"changement contrat assurance\"\n",
        "]\n",
        "\n",
        "tokens = preprocess_text(corpus1)\n",
        "embedding_size = 100\n",
        "trained_embeddings, word2index, index2word = train_skipgram(tokens, embedding_size=embedding_size, epochs=200)\n",
        "\n",
        "# Print the learned word vectors\n",
        "for i, word in index2word.items():\n",
        "    print(f\"Word: {word}, Vector: {trained_embeddings[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8ipgY4o0ahy",
        "outputId": "69e3a914-b8c6-4252-a138-67d9dd290cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 28.961256288313976\n",
            "Epoch 2/200, Loss: 20.234556776496856\n",
            "Epoch 3/200, Loss: 12.497496299224213\n",
            "Epoch 4/200, Loss: 7.114988215133363\n",
            "Epoch 5/200, Loss: 3.9837856054884337\n",
            "Epoch 6/200, Loss: 2.2946752518503475\n",
            "Epoch 7/200, Loss: 1.4220351329607692\n",
            "Epoch 8/200, Loss: 0.9646900977593477\n",
            "Epoch 9/200, Loss: 0.7014407926272845\n",
            "Epoch 10/200, Loss: 0.5354332254296016\n",
            "Epoch 11/200, Loss: 0.4233248216117466\n",
            "Epoch 12/200, Loss: 0.3436963481348283\n",
            "Epoch 13/200, Loss: 0.284928468973719\n",
            "Epoch 14/200, Loss: 0.2402311410976525\n",
            "Epoch 15/200, Loss: 0.2053981679592222\n",
            "Epoch 16/200, Loss: 0.17770027766221516\n",
            "Epoch 17/200, Loss: 0.15529856123962976\n",
            "Epoch 18/200, Loss: 0.13691473027864964\n",
            "Epoch 19/200, Loss: 0.12163649470832828\n",
            "Epoch 20/200, Loss: 0.10879792000779016\n",
            "Epoch 21/200, Loss: 0.09790329660578118\n",
            "Epoch 22/200, Loss: 0.0885772407853157\n",
            "Epoch 23/200, Loss: 0.08053113644348169\n",
            "Epoch 24/200, Loss: 0.07354005095323607\n",
            "Epoch 25/200, Loss: 0.06742653512043838\n",
            "Epoch 26/200, Loss: 0.06204904938697523\n",
            "Epoch 27/200, Loss: 0.05729356130183206\n",
            "Epoch 28/200, Loss: 0.05306735605015885\n",
            "Epoch 29/200, Loss: 0.04929441651828327\n",
            "Epoch 30/200, Loss: 0.04591193298776355\n",
            "Epoch 31/200, Loss: 0.0428676368522788\n",
            "Epoch 32/200, Loss: 0.0401177428958612\n",
            "Epoch 33/200, Loss: 0.037625346153349226\n",
            "Epoch 34/200, Loss: 0.03535916192852436\n",
            "Epoch 35/200, Loss: 0.033292527401459895\n",
            "Epoch 36/200, Loss: 0.03140260446815938\n",
            "Epoch 37/200, Loss: 0.02966973870230415\n",
            "Epoch 38/200, Loss: 0.028076940407920376\n",
            "Epoch 39/200, Loss: 0.02660946186439022\n",
            "Epoch 40/200, Loss: 0.02525445089204019\n",
            "Epoch 41/200, Loss: 0.024000665372814464\n",
            "Epoch 42/200, Loss: 0.022838236758290578\n",
            "Epoch 43/200, Loss: 0.021758473179546815\n",
            "Epoch 44/200, Loss: 0.02075369475058243\n",
            "Epoch 45/200, Loss: 0.019817095181619733\n",
            "Epoch 46/200, Loss: 0.018942625002169988\n",
            "Epoch 47/200, Loss: 0.01812489261836655\n",
            "Epoch 48/200, Loss: 0.01735908015578552\n",
            "Epoch 49/200, Loss: 0.016640871613426794\n",
            "Epoch 50/200, Loss: 0.01596639131111454\n",
            "Epoch 51/200, Loss: 0.015332150977374656\n",
            "Epoch 52/200, Loss: 0.01473500411776763\n",
            "Epoch 53/200, Loss: 0.014172106539982616\n",
            "Epoch 54/200, Loss: 0.013640882103537671\n",
            "Epoch 55/200, Loss: 0.013138992917851115\n",
            "Epoch 56/200, Loss: 0.012664313339903227\n",
            "Epoch 57/200, Loss: 0.012214907227316909\n",
            "Epoch 58/200, Loss: 0.011789007988872325\n",
            "Epoch 59/200, Loss: 0.011385001045747512\n",
            "Epoch 60/200, Loss: 0.011001408375929686\n",
            "Epoch 61/200, Loss: 0.010636874863506687\n",
            "Epoch 62/200, Loss: 0.010290156215709594\n",
            "Epoch 63/200, Loss: 0.00996010824508182\n",
            "Epoch 64/200, Loss: 0.009645677343161404\n",
            "Epoch 65/200, Loss: 0.009345891996529788\n",
            "Epoch 66/200, Loss: 0.009059855216772559\n",
            "Epoch 67/200, Loss: 0.008786737773447503\n",
            "Epoch 68/200, Loss: 0.008525772134079724\n",
            "Epoch 69/200, Loss: 0.008276247027927343\n",
            "Epoch 70/200, Loss: 0.008037502561138539\n",
            "Epoch 71/200, Loss: 0.007808925820239595\n",
            "Epoch 72/200, Loss: 0.007589946908898364\n",
            "Epoch 73/200, Loss: 0.007380035369797883\n",
            "Epoch 74/200, Loss: 0.007178696949400189\n",
            "Epoch 75/200, Loss: 0.0069854706685202805\n",
            "Epoch 76/200, Loss: 0.006799926166084643\n",
            "Epoch 77/200, Loss: 0.0066216612873152645\n",
            "Epoch 78/200, Loss: 0.006450299890944181\n",
            "Epoch 79/200, Loss: 0.0062854898529962995\n",
            "Epoch 80/200, Loss: 0.006126901247238031\n",
            "Epoch 81/200, Loss: 0.005974224684630302\n",
            "Epoch 82/200, Loss: 0.005827169796087103\n",
            "Epoch 83/200, Loss: 0.005685463844564777\n",
            "Epoch 84/200, Loss: 0.005548850454023046\n",
            "Epoch 85/200, Loss: 0.005417088444133564\n",
            "Epoch 86/200, Loss: 0.005289950760790352\n",
            "Epoch 87/200, Loss: 0.00516722349351686\n",
            "Epoch 88/200, Loss: 0.005048704971785813\n",
            "Epoch 89/200, Loss: 0.0049342049330847425\n",
            "Epoch 90/200, Loss: 0.004823543756284015\n",
            "Epoch 91/200, Loss: 0.004716551754509144\n",
            "Epoch 92/200, Loss: 0.004613068522292038\n",
            "Epoch 93/200, Loss: 0.004512942332286678\n",
            "Epoch 94/200, Loss: 0.00441602957729125\n",
            "Epoch 95/200, Loss: 0.004322194253725752\n",
            "Epoch 96/200, Loss: 0.004231307483078973\n",
            "Epoch 97/200, Loss: 0.0041432470681652955\n",
            "Epoch 98/200, Loss: 0.004057897081324831\n",
            "Epoch 99/200, Loss: 0.003975147481963097\n",
            "Epoch 100/200, Loss: 0.0038948937610635633\n",
            "Epoch 101/200, Loss: 0.003817036610518525\n",
            "Epoch 102/200, Loss: 0.0037414816153161228\n",
            "Epoch 103/200, Loss: 0.0036681389667940286\n",
            "Epoch 104/200, Loss: 0.003596923195326535\n",
            "Epoch 105/200, Loss: 0.0035277529209529835\n",
            "Epoch 106/200, Loss: 0.003460550620583355\n",
            "Epoch 107/200, Loss: 0.0033952424105322793\n",
            "Epoch 108/200, Loss: 0.0033317578432377694\n",
            "Epoch 109/200, Loss: 0.0032700297171162516\n",
            "Epoch 110/200, Loss: 0.003209993898591769\n",
            "Epoch 111/200, Loss: 0.00315158915541579\n",
            "Epoch 112/200, Loss: 0.003094757000465877\n",
            "Epoch 113/200, Loss: 0.003039441545276321\n",
            "Epoch 114/200, Loss: 0.0029855893626133203\n",
            "Epoch 115/200, Loss: 0.002933149357461583\n",
            "Epoch 116/200, Loss: 0.002882072645838468\n",
            "Epoch 117/200, Loss: 0.0028323124408973326\n",
            "Epoch 118/200, Loss: 0.002783823945822714\n",
            "Epoch 119/200, Loss: 0.0027365642530581564\n",
            "Epoch 120/200, Loss: 0.002690492249442052\n",
            "Epoch 121/200, Loss: 0.0026455685268584918\n",
            "Epoch 122/200, Loss: 0.0026017552980396493\n",
            "Epoch 123/200, Loss: 0.0025590163171825982\n",
            "Epoch 124/200, Loss: 0.002517316805068422\n",
            "Epoch 125/200, Loss: 0.002476623378393795\n",
            "Epoch 126/200, Loss: 0.0024369039830462918\n",
            "Epoch 127/200, Loss: 0.002398127831073614\n",
            "Epoch 128/200, Loss: 0.002360265341114747\n",
            "Epoch 129/200, Loss: 0.0023232880820773043\n",
            "Epoch 130/200, Loss: 0.0022871687198602933\n",
            "Epoch 131/200, Loss: 0.0022518809669353898\n",
            "Epoch 132/200, Loss: 0.002217399534612747\n",
            "Epoch 133/200, Loss: 0.002183700087828996\n",
            "Epoch 134/200, Loss: 0.0021507592023063704\n",
            "Epoch 135/200, Loss: 0.002118554323941522\n",
            "Epoch 136/200, Loss: 0.002087063730292582\n",
            "Epoch 137/200, Loss: 0.00205626649404123\n",
            "Epoch 138/200, Loss: 0.002026142448314859\n",
            "Epoch 139/200, Loss: 0.0019966721537612924\n",
            "Epoch 140/200, Loss: 0.0019678368672756017\n",
            "Epoch 141/200, Loss: 0.001939618512284699\n",
            "Epoch 142/200, Loss: 0.0019119996505018776\n",
            "Epoch 143/200, Loss: 0.0018849634550684494\n",
            "Epoch 144/200, Loss: 0.0018584936850053805\n",
            "Epoch 145/200, Loss: 0.0018325746609021945\n",
            "Epoch 146/200, Loss: 0.001807191241775141\n",
            "Epoch 147/200, Loss: 0.0017823288030308672\n",
            "Epoch 148/200, Loss: 0.0017579732154754675\n",
            "Epoch 149/200, Loss: 0.0017341108253126926\n",
            "Epoch 150/200, Loss: 0.0017107284350782808\n",
            "Epoch 151/200, Loss: 0.0016878132854608143\n",
            "Epoch 152/200, Loss: 0.0016653530379620609\n",
            "Epoch 153/200, Loss: 0.001643335758352964\n",
            "Epoch 154/200, Loss: 0.0016217499008836656\n",
            "Epoch 155/200, Loss: 0.0016005842932086413\n",
            "Epoch 156/200, Loss: 0.0015798281219900282\n",
            "Epoch 157/200, Loss: 0.0015594709191446145\n",
            "Epoch 158/200, Loss: 0.001539502548701673\n",
            "Epoch 159/200, Loss: 0.0015199131942409978\n",
            "Epoch 160/200, Loss: 0.0015006933468817113\n",
            "Epoch 161/200, Loss: 0.0014818337937948544\n",
            "Epoch 162/200, Loss: 0.001463325607213411\n",
            "Epoch 163/200, Loss: 0.0014451601339154955\n",
            "Epoch 164/200, Loss: 0.001427328985157507\n",
            "Epoch 165/200, Loss: 0.0014098240270353907\n",
            "Epoch 166/200, Loss: 0.0013926373712532896\n",
            "Epoch 167/200, Loss: 0.0013757613662800346\n",
            "Epoch 168/200, Loss: 0.0013591885888749524\n",
            "Epoch 169/200, Loss: 0.0013429118359654891\n",
            "Epoch 170/200, Loss: 0.001326924116859986\n",
            "Epoch 171/200, Loss: 0.0013112186457799745\n",
            "Epoch 172/200, Loss: 0.0012957888346970545\n",
            "Epoch 173/200, Loss: 0.0012806282864601395\n",
            "Epoch 174/200, Loss: 0.001265730788199907\n",
            "Epoch 175/200, Loss: 0.0012510903049975557\n",
            "Epoch 176/200, Loss: 0.00123670097380589\n",
            "Epoch 177/200, Loss: 0.0012225570976113\n",
            "Epoch 178/200, Loss: 0.001208653139825848\n",
            "Epoch 179/200, Loss: 0.0011949837188990342\n",
            "Epoch 180/200, Loss: 0.0011815436031395478\n",
            "Epoch 181/200, Loss: 0.0011683277057376736\n",
            "Epoch 182/200, Loss: 0.0011553310799795142\n",
            "Epoch 183/200, Loss: 0.0011425489146446263\n",
            "Epoch 184/200, Loss: 0.0011299765295790683\n",
            "Epoch 185/200, Loss: 0.0011176093714363022\n",
            "Epoch 186/200, Loss: 0.0011054430095786385\n",
            "Epoch 187/200, Loss: 0.0010934731321324079\n",
            "Epoch 188/200, Loss: 0.0010816955421903245\n",
            "Epoch 189/200, Loss: 0.0010701061541546555\n",
            "Epoch 190/200, Loss: 0.0010587009902154742\n",
            "Epoch 191/200, Loss: 0.0010474761769581255\n",
            "Epoch 192/200, Loss: 0.0010364279420946496\n",
            "Epoch 193/200, Loss: 0.0010255526113139286\n",
            "Epoch 194/200, Loss: 0.0010148466052457223\n",
            "Epoch 195/200, Loss: 0.0010043064365338145\n",
            "Epoch 196/200, Loss: 0.0009939287070139602\n",
            "Epoch 197/200, Loss: 0.000983710104992174\n",
            "Epoch 198/200, Loss: 0.0009736474026195069\n",
            "Epoch 199/200, Loss: 0.0009637374533592223\n",
            "Epoch 200/200, Loss: 0.0009539771895428645\n",
            "Word: assurance, Vector: [ 0.35036393  0.68094459 -0.05275806  0.13490512 -0.10815571  0.82949198\n",
            "  0.33761848 -0.80470609  1.01308024  0.3589914   0.24598668  0.36446746\n",
            " -0.86784264 -0.6665668  -0.28028813  0.18122466  0.96891328  0.80064742\n",
            " -0.87765071 -0.76977121 -0.61102989  0.65375635  0.89434784 -0.71054373\n",
            " -0.50752587 -0.27794038 -0.16845626  0.04561946 -0.11876031  0.72849538\n",
            " -0.00484891 -0.19351008  0.39207326  0.87906702  0.75900442  0.37893288\n",
            "  0.11236108 -0.04905784  0.31010528 -0.43215117  0.34723171 -0.89566227\n",
            " -0.26502526 -0.66628506  0.04918163 -0.3512366  -0.37548217  0.63898179\n",
            " -1.15952793 -0.35219436 -0.44655687  0.10966457  0.76027734 -0.16852067\n",
            " -0.46358105  0.68153124 -0.7484435  -0.25154962 -0.85591313 -0.19450424\n",
            "  0.38805035  0.12655299 -0.48685077  0.60046225  0.20235196  0.19779267\n",
            "  0.79806822  0.10004363  0.90121721  0.9653232  -0.1996505  -0.42455401\n",
            "  0.8411822  -0.73916482 -0.96572128  0.09974975  0.8746898  -0.24232507\n",
            "  0.68740168 -0.9904208   0.96752701 -0.80724312  0.82233684  0.32106947\n",
            " -0.76120812 -0.78700412 -0.14477807 -0.59266485 -0.80441037  0.76267757\n",
            "  0.9547955  -0.22064118 -0.61189838  0.13525993 -0.64060853  0.57535179\n",
            "  0.59004167  0.6328593   0.41890922 -0.15812727]\n",
            "Word: satisfaisant, Vector: [ 0.41931017 -0.50893339 -0.35906273  0.35294106 -0.59185341 -0.56170955\n",
            " -1.15428595 -0.04404184 -0.84951763  0.13917001  0.44163011 -0.18860318\n",
            " -0.50470515  0.367094   -0.39036513  0.8860291  -0.56498937  0.24618171\n",
            " -0.7591933   0.26948032  0.91597596 -0.96606195  0.12230797 -0.12571843\n",
            " -0.21581137  0.10857971 -0.24577816  0.3180049   0.20326003 -0.71671844\n",
            "  0.96783143  0.80822795 -0.93230639 -0.48630128  0.4347264   0.71512463\n",
            "  0.83874148  0.23416029 -0.7921655  -0.35712151 -0.32323949  0.34613608\n",
            "  0.61574501  0.52554648  1.01359473  0.83246026 -0.19629752  0.44949611\n",
            "  0.42114077  0.08850187 -0.02233876 -0.42266638  0.14552263  0.55394168\n",
            " -0.45755314 -0.44213988 -0.43563913  0.57563308 -0.76199875  0.85531513\n",
            " -0.21677237  0.35982904 -0.47534055  0.55082627 -0.80483279  0.64879417\n",
            " -0.25713665  0.77321967 -0.6799084  -0.76789119  0.80083     0.34243816\n",
            "  0.03992663  0.93272922 -0.69330079 -0.70990238 -0.80029111  0.22487722\n",
            "  0.34400408  0.35157979 -0.85270853  0.65228675 -0.12120208 -0.59358674\n",
            "  0.80984352 -0.04932653 -0.37140261 -0.19145923  0.18440679 -0.0090019\n",
            " -0.80087184 -0.51531573 -0.38114904  0.26699792 -0.49393084  0.59467248\n",
            " -0.21086152 -0.06273909 -0.05801189  0.46595197]\n",
            "Word: changement, Vector: [ 0.27345583  0.09586528  0.59849409  0.21222309  0.83008014  0.21105963\n",
            " -1.11067395 -0.46991133  0.39439381  1.27730845 -0.31988839  0.07521629\n",
            " -0.16667107  0.4065397   0.97063267  0.73080378 -1.21103727  0.51871561\n",
            " -0.8985217   0.42822659 -0.58788039  0.91248798  0.37956908 -0.03732348\n",
            " -0.15669292  1.03187613  1.09198385  0.90535437 -0.7581631  -0.76435448\n",
            " -0.36310762 -0.11246171  0.96115903  0.75222616 -0.41231753  0.40293644\n",
            "  0.70762311  0.69633139 -0.08139667  0.49409276  0.03188065 -0.57009868\n",
            " -0.41897859  0.31606659  0.62811239 -0.87059915  0.51522996 -0.36696904\n",
            "  0.98020185  0.45736622  0.81511135  0.20745657 -1.06784345  0.45619467\n",
            "  0.80625031  0.70287259 -0.18121587 -0.07870326  0.25848195  0.78750983\n",
            " -0.48392592  1.01848468 -0.53457202  0.89184124  0.36169919 -0.2033663\n",
            "  0.85591211  0.27925013  0.72669648  0.04397494  0.58942107 -0.42033975\n",
            " -0.81475024  0.58312317 -0.78828674 -0.27225981  0.049017   -0.03242437\n",
            " -0.87549063 -0.8163791   0.74929066 -0.3206694   0.35186011  0.57279986\n",
            "  0.91122117 -0.60232451  0.42717812 -0.22166296  0.10873212  1.12087893\n",
            "  0.40979535  0.16855155  0.48228698 -0.02805309  0.16839213 -0.22114734\n",
            "  0.30895713  0.83685619  1.03141383 -0.60890243]\n",
            "Word: trouver, Vector: [ 0.08057698  0.73430872  0.8405207  -0.48227008 -0.29069724  0.01574589\n",
            " -0.66845276 -0.36831437  0.35888738 -0.65458025  0.12697289  1.06654398\n",
            "  0.72233376  0.54220776  0.6517845   0.93620541  0.65125504 -0.52218327\n",
            "  0.77437091  0.93379692 -0.4080199   0.79375931 -0.66357387 -0.39456167\n",
            "  0.55647238 -0.3202179  -0.2139174   0.56553739 -0.26208308  0.89441037\n",
            "  1.07837815 -0.0147288  -0.21341239  0.04498463 -0.24629378 -0.14887791\n",
            " -0.56148591  0.67618369  1.0994373  -0.45448648  0.31545468  0.84164727\n",
            " -0.56176904  0.19416932  0.84055177 -0.48516646 -0.14339774  0.96088476\n",
            "  0.81176266  0.45159457 -0.19985677  0.19282609  0.17909262  0.5806914\n",
            "  0.71791283  0.52772595 -0.45576733  0.43111934  0.62115878  0.09087181\n",
            " -0.18661635  0.82513329  0.12928855  0.7767085   0.77209721  0.50062044\n",
            " -0.38812858  0.21073819  1.13002265 -0.39088437 -0.73565384 -0.1835968\n",
            "  1.07437644 -0.12864389 -0.27665655 -1.00188386  0.4210032  -0.26030513\n",
            "  0.54889004  0.89415781 -1.09899174 -0.16811136  0.70746607  0.10756644\n",
            "  0.72624464  0.33108107 -0.47442067 -0.19575046 -0.42250078  0.32470026\n",
            "  0.14204138  0.18216401 -0.38910917 -1.02155769 -0.84879845  0.36005047\n",
            " -0.43997957  0.97039701 -0.17270436 -0.5577304 ]\n",
            "Word: bonne, Vector: [ 0.16896042 -0.68761718  0.09040563  0.92091916 -0.52911645 -0.23052157\n",
            "  0.36482121  0.89905202 -0.12343782  0.14039084  0.11420812 -0.23977833\n",
            "  0.411199   -0.39033915  0.1002476  -0.20370242 -0.86541054 -0.0951642\n",
            " -0.88173863  0.67587835  0.23453223 -0.56647299  0.27005272  1.39916423\n",
            "  0.04740552  0.64604171  0.4492911  -0.57540598 -0.49358242 -0.49547924\n",
            "  1.00024936  0.37279486  1.02092055 -0.71702534 -0.80247097 -0.20430451\n",
            "  0.93311664  0.10663945  0.67960929  0.25870459  0.97711652  0.40688559\n",
            " -0.32624763  0.54189354  0.36276805  0.15527141 -0.18191942 -0.8428138\n",
            "  0.4929964   0.81299509  0.74933181 -0.43358809  0.26294994  0.88062938\n",
            " -0.75335546 -0.61270615  0.23032771 -0.01644922 -0.505255   -1.03933454\n",
            " -0.50808455  0.62564251 -0.06224919  0.0124491  -0.79887741  0.78466516\n",
            " -0.38682706 -0.61761741  0.39018647 -0.16937677  0.20142497  0.09430081\n",
            " -0.28811046  0.71309071 -0.14128969  0.13303968  1.04489786 -0.46825213\n",
            "  0.39997681 -0.45390835 -0.45535517  0.56668071 -0.33560986  0.90335371\n",
            " -0.94425259 -0.57786689 -0.91805906  0.21939281  0.2973459  -0.05449376\n",
            "  0.07903509 -0.56775688  0.4912889  -0.43908077 -0.06237822 -0.84617646\n",
            "  0.37608922 -0.31039005 -0.35280867 -0.66779191]\n",
            "Word: contrat, Vector: [ 0.37662697  0.58488892  0.79110135  0.80813496  0.98586308 -0.56757199\n",
            " -0.94112649  0.31981976 -1.0207619  -0.86622026  0.1222143   0.70085521\n",
            "  0.38285354 -0.37490552 -0.4683363   0.26974892 -0.82552125 -0.10588126\n",
            "  0.16986891  0.33115098  0.36783353 -0.0642212   0.46458612 -0.39127544\n",
            "  0.01628469  0.84023147  0.46057392 -0.10175572  0.72782094  0.41462423\n",
            "  1.00065879  0.32928656  0.47721782 -0.9801521   0.75100841  0.616292\n",
            " -0.9899745   0.97066355  0.63421433 -1.09147084 -0.63854886 -0.74910464\n",
            "  0.75870085  0.51043924 -0.7770038   0.55768885  0.7048328  -0.82454698\n",
            "  0.08134813  0.53448061 -0.31746521  0.8570273  -0.39030721 -0.41874195\n",
            "  0.13074935  0.07231916 -0.93742038  0.4467847  -0.65793119 -0.23480037\n",
            " -0.06512231  0.44002362  0.08308744  0.4385324   1.10490572  0.11116637\n",
            " -0.07408219 -0.17980264  0.56998513  1.11146297  0.4952387  -0.68977944\n",
            " -0.49396778 -0.61817618 -0.37625643  0.37842528  0.19231928  0.90690065\n",
            " -0.43737594  0.98012844  0.24479111 -0.34514214 -0.07887343  0.02055183\n",
            "  0.39565692  0.37682705 -0.13251828  0.57097954 -0.59933823  0.31880778\n",
            "  0.69619736 -0.55308251 -0.79524141  0.47579294 -0.06862273 -0.46372024\n",
            " -0.1491101  -0.42727345  0.12569721  0.01669599]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=preprocess_text(corpus)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHLpBixPpik7",
        "outputId": "c74f787b-f523-4002-c1e4-5560c9b26037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['trouver', 'bonne', 'assurance'],\n",
              " ['contrat', 'satisfaisant'],\n",
              " ['changement', 'contrat', 'assurance']]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs= generate_skipgram_pairs(tokens,2)\n",
        "pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "babSXyyEp0IN",
        "outputId": "6711f9c9-8125-4e1f-a9d4-aafb7f9e851d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('trouver', 'bonne'),\n",
              " ('trouver', 'assurance'),\n",
              " ('bonne', 'trouver'),\n",
              " ('bonne', 'assurance'),\n",
              " ('assurance', 'trouver'),\n",
              " ('assurance', 'bonne'),\n",
              " ('contrat', 'satisfaisant'),\n",
              " ('satisfaisant', 'contrat'),\n",
              " ('changement', 'contrat'),\n",
              " ('changement', 'assurance'),\n",
              " ('contrat', 'changement'),\n",
              " ('contrat', 'assurance'),\n",
              " ('assurance', 'changement'),\n",
              " ('assurance', 'contrat')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate cosine similarity between two vectors\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    similarity = dot_product / (norm1 * norm2)\n",
        "    return similarity\n",
        "# Test word similarity\n",
        "def test_word_similarity(word1, word2, word_vectors, word2index):\n",
        "    vec1 = word_vectors[word2index[word1]]\n",
        "    vec2 = word_vectors[word2index[word2]]\n",
        "    similarity = cosine_similarity(vec1, vec2)\n",
        "    return similarity\n",
        "# Test the similarity of words\n",
        "similarity_result = test_word_similarity(\"trouver\", \"assurance\", trained_embeddings, word2index)\n",
        "print(f\"Similarity between 'trouver' and 'assurance': {similarity_result}\")\n",
        "\n",
        "similarity_result = test_word_similarity(\"contrat\", \"assurance\", trained_embeddings, word2index)\n",
        "print(f\"Similarity between 'contrat' and 'assurance': {similarity_result}\")\n",
        "\n",
        "similarity_result = test_word_similarity(\"trouver\", \"contrat\", trained_embeddings, word2index)\n",
        "print(f\"Similarity between 'trouver' and 'contrat': {similarity_result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5ih19kM3xHw",
        "outputId": "322d078c-8631-4931-8324-8efec9a8b6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'trouver' and 'assurance': -0.1157287408436277\n",
            "Similarity between 'contrat' and 'assurance': -0.050230775882811815\n",
            "Similarity between 'trouver' and 'contrat': -0.04861483230841686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ROTktdu3_c8",
        "outputId": "6b70a527-834a-4f65-d540-8d69f7a5e8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Your corpus\n",
        "corpus1 = [\n",
        "    \"trouver bonne assurance\",\n",
        "    \"contrat satisfaisant\",\n",
        "    \"changement contrat assurance\"\n",
        "]\n",
        "nltk.download('stopwords')\n",
        "# Tokenize and preprocess the corpus\n",
        "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus1]\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('french'))\n",
        "tokenized_corpus = [[word for word in sentence if word not in stop_words] for sentence in tokenized_corpus]\n",
        "\n",
        "# Set up and train the Skip-gram model\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=1, min_count=1)\n",
        "\n",
        "# Save the trained model to a file\n",
        "model.save(\"skip_gram_model\")\n",
        "# Print the learned word vectors\n",
        "word_vectors = model.wv\n",
        "for word in word_vectors.key_to_index:\n",
        "    print(f\"Word: {word}, Vector: {word_vectors[word]}\")\n",
        "\n",
        "# Calculate and print cosine similarity between all pairs of words in the vocabulary\n",
        "vocab_words = list(word_vectors.key_to_index.keys())\n",
        "\n",
        "for i in range(len(vocab_words)):\n",
        "    for j in range(i + 1, len(vocab_words)):\n",
        "        word1 = vocab_words[i]\n",
        "        word2 = vocab_words[j]\n",
        "        similarity_result = cosine_similarity([word_vectors[word1]], [word_vectors[word2]])[0][0]\n",
        "        print(f\"Cosine Similarity between '{word1}' and '{word2}': {similarity_result}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ctHF6Kw4CS5",
        "outputId": "95dcd159-18ca-4d4c-d0de-eff310d8811b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: contrat, Vector: [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
            " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
            " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
            " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
            "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
            "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
            "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
            " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
            "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
            "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
            " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
            " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
            "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
            " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
            "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
            " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
            " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
            " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
            " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
            "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
            " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
            " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
            " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
            "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
            " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
            "Word: assurance, Vector: [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
            "Word: changement, Vector: [ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
            "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
            " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
            "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
            "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
            " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
            "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
            " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
            " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
            " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
            " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
            "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
            " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
            " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
            " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
            "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
            " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
            " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
            "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
            "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
            "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
            "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
            "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
            "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
            "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
            "Word: satisfaisant, Vector: [-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
            "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
            "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
            "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
            "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
            "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
            " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
            "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
            " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
            "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
            "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
            "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
            " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
            " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
            "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
            "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
            "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
            " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
            " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
            " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
            " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
            " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
            " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
            " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
            "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n",
            "Word: bonne, Vector: [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
            "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
            " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
            "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
            " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
            "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
            " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
            " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
            "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
            " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
            "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
            "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
            " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
            "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
            " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
            "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
            "  0.00620989  0.00481889  0.00078719  0.00301345]\n",
            "Word: trouver, Vector: [-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
            " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
            " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
            " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
            " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
            " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
            " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
            "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
            " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
            "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
            "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
            " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
            " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
            "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
            "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
            "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
            "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
            " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
            "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
            " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
            "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
            " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
            " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
            " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
            " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n",
            "Cosine Similarity between 'contrat' and 'assurance': -0.010839177295565605\n",
            "Cosine Similarity between 'contrat' and 'changement': -0.052346739917993546\n",
            "Cosine Similarity between 'contrat' and 'satisfaisant': -0.1116705909371376\n",
            "Cosine Similarity between 'contrat' and 'bonne': -0.027750365436077118\n",
            "Cosine Similarity between 'contrat' and 'trouver': -0.05987630411982536\n",
            "Cosine Similarity between 'assurance' and 'changement': -0.02367165870964527\n",
            "Cosine Similarity between 'assurance' and 'satisfaisant': 0.06797594577074051\n",
            "Cosine Similarity between 'assurance' and 'bonne': 0.004503008909523487\n",
            "Cosine Similarity between 'assurance' and 'trouver': 0.009391160681843758\n",
            "Cosine Similarity between 'changement' and 'satisfaisant': -0.013514944352209568\n",
            "Cosine Similarity between 'changement' and 'bonne': 0.17018885910511017\n",
            "Cosine Similarity between 'changement' and 'trouver': 0.06408978998661041\n",
            "Cosine Similarity between 'satisfaisant' and 'bonne': -0.044617101550102234\n",
            "Cosine Similarity between 'satisfaisant' and 'trouver': 0.1314900517463684\n",
            "Cosine Similarity between 'bonne' and 'trouver': 0.13887983560562134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    }
  ]
}